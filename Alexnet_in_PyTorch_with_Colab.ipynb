{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os # 파이썬을 이용해 파일을 복사하거나 디렉터리를 생성하고 특정 디렉터리 내의 파일 목록을 구하고자 할 때 사용\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torchvision # torchvision package : 컴퓨터 비전을 위한 유명 데이터셋, 모델 아키텍처, 이미지 변형등을 포함\n",
        "import torch.nn as nn # nn : neural netwroks (define class) attribute를 활용해 state를 저장하고 활용\n",
        "import torch.optim as optim # 최적화 알고리즘\n",
        "import torch.nn.functional as F # (define function) 인스턴스화 시킬 필요없이 사용 가능\n",
        "from PIL import Image\n",
        "from torchvision import transforms, datasets # transforms : 데이터를 조작하고 학습에 적합하게 만듦.\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "# dataset : 샘플과 정답(label)을 저장\n",
        "# DataLoader : Dataset 을 샘플에 쉽게 접근할 수 있도록 순회 가능한 객체(iterable)로 감싼다."
      ],
      "metadata": {
        "id": "mcDN5t8dXHJb"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 10 # 훈련 반복수\n",
        "batch_size = 512 # 배치 크기\n",
        "\n",
        "device = (\"cuda\" if torch.cuda.is_available() else \"cpu\") # device 정의\n",
        "class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
        "               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot'] # 총 10개의 클래스\n",
        "\n",
        "print(torch.__version__)\n",
        "print(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FNb7DpvIaXZc",
        "outputId": "8e890dd6-b8b0-46ec-cb24-184be3277922"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.0.1+cu118\n",
            "cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.Resize(227), # Compose : transforms 리스트 구성\n",
        "    # 227x227 : input image(in alexnet) but fashionMNIST's input image : 28x28\n",
        "    transforms.ToTensor()]) # ToTensor : PIL image or numpy.ndarray를 tensor로 바꿈 # 딥러닝 모델에 데이터 입력하기 위해서는 텐서로 변환해야 함\n",
        "\n",
        "training_data = datasets.FashionMNIST(\n",
        "    root=\"data\", # data가 저장될 경로(path)\n",
        "    train=True, # training dataset\n",
        "    download=True, # 인터넷으로부터 데이터 다운\n",
        "    transform=transform # feature 및 label 변환(transformation) 지정\n",
        ")\n",
        "\n",
        "validation_data = datasets.FashionMNIST(\n",
        "    root=\"data\",\n",
        "    train=False, # test dataset\n",
        "    download=True,\n",
        "    transform=transform\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LhB_KRxxYlVX",
        "outputId": "c1cd50f3-22f7-410a-d05a-92491a23d050"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to data/FashionMNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 26421880/26421880 [00:02<00:00, 11824493.21it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/FashionMNIST/raw/train-images-idx3-ubyte.gz to data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to data/FashionMNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 29515/29515 [00:00<00:00, 210231.25it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/FashionMNIST/raw/train-labels-idx1-ubyte.gz to data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4422102/4422102 [00:01<00:00, 3760385.73it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5148/5148 [00:00<00:00, 21086208.00it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to data/FashionMNIST/raw\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# (class) DataLoader(dataset, batch_size, shuffle, ...)\n",
        "# DataLoader는 주어진 데이터셋을 batch로 나누고 데이터를 미니배치로 제공\n",
        "training_loader = DataLoader(training_data, batch_size=64, shuffle=True)\n",
        "validation_loader = DataLoader(validation_data, batch_size=64, shuffle=True)"
      ],
      "metadata": {
        "id": "jysp0OrkYoB0"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# DataLoader에서 첫 번째 미니배치 가져오기\n",
        "first_batch = next(iter(training_loader))\n",
        "\n",
        "# 미니배치의 데이터와 레이블 가져오기\n",
        "images, labels = first_batch\n",
        "\n",
        "# 미니배치의 크기 확인\n",
        "print(images.shape)  # 이미지 데이터의 크기 출력\n",
        "print(labels.shape)  # 레이블 데이터의 크기 출력"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Or4Yus9ALU6",
        "outputId": "edf9ef45-b733-467b-88aa-58c74b3b3de5"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([64, 1, 227, 227])\n",
            "torch.Size([64])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "iter(training_loader)를 호출하여 training_loader 데이터 로더를 반복 가능한(iterable) 객체로 변환합니다. 이를 통해 데이터 로더의 각 미니배치에 순차적으로 액세스할 수 있는 반복자(iterator)를 생성합니다.\n",
        "\n",
        "next(iter(training_loader))를 호출하여 반복자에서 다음 미니배치를 가져옵니다. 이 작업은 데이터 로더에서 다음 미니배치를 로드하는 역할을 합니다.\n",
        "\n",
        "first_batch 변수에 이 미니배치를 저장합니다. 이 미니배치는 텐서로 구성된 이미지 데이터와 해당 이미지의 레이블 데이터를 포함하고 있습니다.\n",
        "\n",
        "즉, 위의 코드는 training_loader로부터 첫 번째 미니배치를 가져와서 first_batch 변수에 저장하는 것입니다. 이렇게 하면 첫 번째 미니배치의 데이터를 사용하여 모델을 학습하거나 분석할 수 있습니다. 첫 번째 미니배치 이후에는 다음 미니배치를 가져오기 위해 다시 next(iter(training_loader))를 호출할 수 있습니다."
      ],
      "metadata": {
        "id": "BBDem59oA3uf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class fashion_mnist_alexnet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Sequential(\n",
        "\n",
        "            nn.Conv2d(in_channels=1, out_channels=96, kernel_size=11, stride=4, padding=0),\n",
        "            # 4D tensor : [number_of_kernels, input_channels, kernel_width, kernel_height]\n",
        "            # = 96x1x11x11\n",
        "            # input size : 1x227x227\n",
        "            # input size 정의 : (N, C, H, W) or (C, H, W)\n",
        "            # W' = (W-F+2P)/S + 1\n",
        "            # 55x55x96 feature map 생성 (55는 (227-11+1)/4)\n",
        "            # 최종적으로 227 -> 55\n",
        "            nn.ReLU(), # 96x55x55\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2)\n",
        "            # 55 -> (55-3+1)/2 = 26.5 = 27\n",
        "            # 96x27x27 feature map 생성\n",
        "\n",
        "        )\n",
        "        self.conv2 = nn.Sequential(\n",
        "            nn.Conv2d(96, 256, 5, 1, 2), # in_channels: 96, out_channels: 256, kernel_size=5x5, stride=1, padding=2\n",
        "            # kernel 수 = 48x5x5 (드롭아웃을 사용했기 때문에 96/2=48) 형태의 256개\n",
        "            # 256x27x27\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(3, 2) # 27 -> 13\n",
        "            # 256x13x13\n",
        "        )\n",
        "        self.conv3 = nn.Sequential(\n",
        "            nn.Conv2d(256, 384, 3, 1, 1),\n",
        "            nn.ReLU() # 13 유지\n",
        "            # 384x13x13\n",
        "        )\n",
        "        self.conv4 = nn.Sequential(\n",
        "            nn.Conv2d(384, 384, 3, 1, 1),\n",
        "            nn.ReLU() # 13 유지\n",
        "            # 384x13x13\n",
        "        )\n",
        "        self.conv5 = nn.Sequential(\n",
        "            nn.Conv2d(384, 256, 3, 1, 1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(3, 2) # 13 -> 6\n",
        "            # 256x6x6\n",
        "        )\n",
        "\n",
        "        self.fc1 = nn.Linear(256 * 6 * 6, 4096)\n",
        "        self.fc2 = nn.Linear(4096, 4096)\n",
        "        self.fc3 = nn.Linear(4096, 10)\n",
        "\n",
        "    def forward(self, x): # input size = 3x227x227\n",
        "        out = self.conv1(x)\n",
        "        out = self.conv2(out)\n",
        "        out = self.conv3(out)\n",
        "        out = self.conv4(out)\n",
        "        out = self.conv5(out) # 64x4096x1x1\n",
        "        out = out.view(out.size(0), -1) # 64x4096\n",
        "\n",
        "        out = F.relu(self.fc1(out))\n",
        "        out = F.dropout(out, 0.5)\n",
        "        out = F.relu(self.fc2(out))\n",
        "        out = F.dropout(out, 0.5)\n",
        "        out = self.fc3(out)\n",
        "        out = F.log_softmax(out, dim=1)\n",
        "\n",
        "        return out"
      ],
      "metadata": {
        "id": "sSgG7QnJYyHH"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = fashion_mnist_alexnet().to(device) # to()로 모델에 gpu 사용\n",
        "criterion = F.nll_loss # nll_loss : negative log likelihood loss\n",
        "optimizer = optim.Adam(model.parameters()) # model(신경망) 파라미터를 optimizer에 전달해줄 때 nn.Module의 parameters() 메소드를 사용"
      ],
      "metadata": {
        "id": "FN7KTstugMs6"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchsummary import summary as summary_\n",
        "\n",
        "summary_(model, (1,227,227), batch_size)\n",
        "# summary_: (model, input_size, batch_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vvee-485Z9Td",
        "outputId": "4f2f255a-163a-4ac8-a29c-1564ceb8d61e"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1          [512, 96, 55, 55]          11,712\n",
            "              ReLU-2          [512, 96, 55, 55]               0\n",
            "         MaxPool2d-3          [512, 96, 27, 27]               0\n",
            "            Conv2d-4         [512, 256, 27, 27]         614,656\n",
            "              ReLU-5         [512, 256, 27, 27]               0\n",
            "         MaxPool2d-6         [512, 256, 13, 13]               0\n",
            "            Conv2d-7         [512, 384, 13, 13]         885,120\n",
            "              ReLU-8         [512, 384, 13, 13]               0\n",
            "            Conv2d-9         [512, 384, 13, 13]       1,327,488\n",
            "             ReLU-10         [512, 384, 13, 13]               0\n",
            "           Conv2d-11         [512, 256, 13, 13]         884,992\n",
            "             ReLU-12         [512, 256, 13, 13]               0\n",
            "        MaxPool2d-13           [512, 256, 6, 6]               0\n",
            "           Linear-14                [512, 4096]      37,752,832\n",
            "           Linear-15                [512, 4096]      16,781,312\n",
            "           Linear-16                  [512, 10]          40,970\n",
            "================================================================\n",
            "Total params: 58,299,082\n",
            "Trainable params: 58,299,082\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 100.64\n",
            "Forward/backward pass size (MB): 5589.16\n",
            "Params size (MB): 222.39\n",
            "Estimated Total Size (MB): 5912.20\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, device, train_loader, optimizer, epoch):\n",
        "    model.train() # 모델을 학습 모드로 설정 # 모델 내부에서 드롭아웃 및 배치 정규와 등 활성화 되도록\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        # enumberate() : 인덱스와 원소로 이루어진 튜플(tuple)을 만들어줌\n",
        "        target = target.type(torch.LongTensor) # 모델의 출력과 타겟 간의 데이터 형식을 일치시키기 위해 필요함\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        optimizer.zero_grad() # 항상 backpropagation 하기전에 미분(gradient)을 zero로 만들어주고 시작해야 한다.\n",
        "        # 각 배치마다 모델의 파라미터 gd를 누적하는 것을 방지\n",
        "        output = model(data)\n",
        "        loss = criterion(output, target) # criterion = loss_fn\n",
        "        loss.backward() # Computes the gradient of current tensor w.r.t. graph leaves\n",
        "        optimizer.step() # step() : 파라미터를 업데이트함 # 경사 하강법 수행하는 단계\n",
        "        if (batch_idx + 1) % 30 == 0: # 30번째 미니배치 마다 현재 학습 상황을 출력\n",
        "            print(\"Train Epoch:{} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\".format(\n",
        "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
        "                100. * batch_idx / len(train_loader), loss.item()))"
      ],
      "metadata": {
        "id": "fcAVGT7AY1gk"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test(model, device, test_loader):\n",
        "    model.eval() # 모델을 평가 모드로 설정 # 모델 내부에서 드롭아웃 및 배치 정규화 등 비활성화 되도록\n",
        "    test_loss = 0\n",
        "    correct = 0 # 정확도 초기화\n",
        "    with torch.no_grad(): # test 중에는 기울기 계산 안해도 되니까 # 메모리 절약 가능\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            test_loss += criterion(output, target, reduction='sum').item() #reduction='sum' 은 각 미니배치에서 손실을 모두 더한다는 것\n",
        "            pred = output.max(1, keepdim=True)[1] # 모델의 출력 중 가장 큰 값의 인덱스 사용하여 예측 값 계산\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item() # 맞은 예측의 개수\n",
        "\n",
        "        test_loss /= len(test_loader.dataset)  # -> mean\n",
        "        print(\"\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n\".format(\n",
        "            test_loss, correct, len(test_loader.dataset), 100. * correct / len(test_loader.dataset)))\n",
        "        print('='*50)"
      ],
      "metadata": {
        "id": "NO1MtJ6lY3in"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###pred = output.max(1, keepdim=True)[1]\n",
        "\n",
        "- output.max(1, keepdim=True)[1]은 각 이미지에 대해 가장 큰 값을 찾고 그 값을 (값, 인덱스) 형태로 반환, keepdim=True는 차원 유지 상태로 값 반환\n",
        "- [1]는 반환된 튜플의 두 번째 요소, 즉 가장 큰 값의 인덱스 선택\n",
        "- 따라서 pred는 모델의 예측값 중 가장 큰 값을 갖는 클래스의 인덱스를 담고 있는 리스트!!\n",
        "- 가장 큰 값 == 확률이 가장 높은 것\n",
        "\n",
        "---------\n",
        "\n",
        "### 실제 클래스 레이블(target)과 모델의 예측 결과(pred) 예시\n",
        "target = [2, 0, 1, 4, 3]  # 각 이미지의 실제 클래스 레이블\n",
        "pred = [2, 0, 1, 3, 3]    # 모델의 예측 결과\n",
        "\n",
        "---------\n",
        "\n",
        "###correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "- pred.eq(target.view_as(pred))는 pred와 target 비교한 boolean 값 리스트 생성\n",
        "- sum으로 true 개수 더하고\n",
        "- item으로 파이썬의 정수로 변호나\n",
        "\n",
        "---------\n",
        "\n",
        "따라서 예시 기준으로 코드를 수행하면\n",
        "pred.eq(target.view_as(pred)) =>  [True, True, True, False, True]"
      ],
      "metadata": {
        "id": "ZbravTeSF_J4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(1, epochs+1):\n",
        "    train(model, device, training_loader, optimizer, epoch)\n",
        "    test(model, device, validation_loader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "USdc8dyPZM1w",
        "outputId": "019e55bc-9dcd-4551-8a4a-401349f9e46c"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Epoch:1 [1856/60000 (3%)]\tLoss: 0.242897\n",
            "Train Epoch:1 [3776/60000 (6%)]\tLoss: 0.156035\n",
            "Train Epoch:1 [5696/60000 (9%)]\tLoss: 0.102052\n",
            "Train Epoch:1 [7616/60000 (13%)]\tLoss: 0.305565\n",
            "Train Epoch:1 [9536/60000 (16%)]\tLoss: 0.173029\n",
            "Train Epoch:1 [11456/60000 (19%)]\tLoss: 0.026302\n",
            "Train Epoch:1 [13376/60000 (22%)]\tLoss: 0.313172\n",
            "Train Epoch:1 [15296/60000 (25%)]\tLoss: 0.127456\n",
            "Train Epoch:1 [17216/60000 (29%)]\tLoss: 0.122151\n",
            "Train Epoch:1 [19136/60000 (32%)]\tLoss: 0.112499\n",
            "Train Epoch:1 [21056/60000 (35%)]\tLoss: 0.151660\n",
            "Train Epoch:1 [22976/60000 (38%)]\tLoss: 0.112900\n",
            "Train Epoch:1 [24896/60000 (41%)]\tLoss: 0.191683\n",
            "Train Epoch:1 [26816/60000 (45%)]\tLoss: 0.175339\n",
            "Train Epoch:1 [28736/60000 (48%)]\tLoss: 0.253973\n",
            "Train Epoch:1 [30656/60000 (51%)]\tLoss: 0.180366\n",
            "Train Epoch:1 [32576/60000 (54%)]\tLoss: 0.096055\n",
            "Train Epoch:1 [34496/60000 (57%)]\tLoss: 0.298041\n",
            "Train Epoch:1 [36416/60000 (61%)]\tLoss: 0.157342\n",
            "Train Epoch:1 [38336/60000 (64%)]\tLoss: 0.115836\n",
            "Train Epoch:1 [40256/60000 (67%)]\tLoss: 0.134209\n",
            "Train Epoch:1 [42176/60000 (70%)]\tLoss: 0.189229\n",
            "Train Epoch:1 [44096/60000 (73%)]\tLoss: 0.169071\n",
            "Train Epoch:1 [46016/60000 (77%)]\tLoss: 0.155891\n",
            "Train Epoch:1 [47936/60000 (80%)]\tLoss: 0.076829\n",
            "Train Epoch:1 [49856/60000 (83%)]\tLoss: 0.183494\n",
            "Train Epoch:1 [51776/60000 (86%)]\tLoss: 0.124933\n",
            "Train Epoch:1 [53696/60000 (89%)]\tLoss: 0.223788\n",
            "Train Epoch:1 [55616/60000 (93%)]\tLoss: 0.155645\n",
            "Train Epoch:1 [57536/60000 (96%)]\tLoss: 0.265531\n",
            "Train Epoch:1 [59456/60000 (99%)]\tLoss: 0.162512\n",
            "\n",
            "Test set: Average loss: 0.2889, Accuracy: 9074/10000 (91%)\n",
            "\n",
            "==================================================\n",
            "Train Epoch:2 [1856/60000 (3%)]\tLoss: 0.227840\n",
            "Train Epoch:2 [3776/60000 (6%)]\tLoss: 0.188862\n",
            "Train Epoch:2 [5696/60000 (9%)]\tLoss: 0.295785\n",
            "Train Epoch:2 [7616/60000 (13%)]\tLoss: 0.326426\n",
            "Train Epoch:2 [9536/60000 (16%)]\tLoss: 0.140479\n",
            "Train Epoch:2 [11456/60000 (19%)]\tLoss: 0.196092\n",
            "Train Epoch:2 [13376/60000 (22%)]\tLoss: 0.136982\n",
            "Train Epoch:2 [15296/60000 (25%)]\tLoss: 0.143432\n",
            "Train Epoch:2 [17216/60000 (29%)]\tLoss: 0.207381\n",
            "Train Epoch:2 [19136/60000 (32%)]\tLoss: 0.139575\n",
            "Train Epoch:2 [21056/60000 (35%)]\tLoss: 0.193274\n",
            "Train Epoch:2 [22976/60000 (38%)]\tLoss: 0.045215\n",
            "Train Epoch:2 [24896/60000 (41%)]\tLoss: 0.164996\n",
            "Train Epoch:2 [26816/60000 (45%)]\tLoss: 0.355211\n",
            "Train Epoch:2 [28736/60000 (48%)]\tLoss: 0.122103\n",
            "Train Epoch:2 [30656/60000 (51%)]\tLoss: 0.071283\n",
            "Train Epoch:2 [32576/60000 (54%)]\tLoss: 0.263239\n",
            "Train Epoch:2 [34496/60000 (57%)]\tLoss: 0.163381\n",
            "Train Epoch:2 [36416/60000 (61%)]\tLoss: 0.152561\n",
            "Train Epoch:2 [38336/60000 (64%)]\tLoss: 0.239368\n",
            "Train Epoch:2 [40256/60000 (67%)]\tLoss: 0.094221\n",
            "Train Epoch:2 [42176/60000 (70%)]\tLoss: 0.120212\n",
            "Train Epoch:2 [44096/60000 (73%)]\tLoss: 0.084252\n",
            "Train Epoch:2 [46016/60000 (77%)]\tLoss: 0.091627\n",
            "Train Epoch:2 [47936/60000 (80%)]\tLoss: 0.134000\n",
            "Train Epoch:2 [49856/60000 (83%)]\tLoss: 0.338473\n",
            "Train Epoch:2 [51776/60000 (86%)]\tLoss: 0.322261\n",
            "Train Epoch:2 [53696/60000 (89%)]\tLoss: 0.222851\n",
            "Train Epoch:2 [55616/60000 (93%)]\tLoss: 0.290678\n",
            "Train Epoch:2 [57536/60000 (96%)]\tLoss: 0.214254\n",
            "Train Epoch:2 [59456/60000 (99%)]\tLoss: 0.132704\n",
            "\n",
            "Test set: Average loss: 0.3157, Accuracy: 9070/10000 (91%)\n",
            "\n",
            "==================================================\n",
            "Train Epoch:3 [1856/60000 (3%)]\tLoss: 0.096324\n",
            "Train Epoch:3 [3776/60000 (6%)]\tLoss: 0.165027\n",
            "Train Epoch:3 [5696/60000 (9%)]\tLoss: 0.110640\n",
            "Train Epoch:3 [7616/60000 (13%)]\tLoss: 0.135381\n",
            "Train Epoch:3 [9536/60000 (16%)]\tLoss: 0.176475\n",
            "Train Epoch:3 [11456/60000 (19%)]\tLoss: 0.134389\n",
            "Train Epoch:3 [13376/60000 (22%)]\tLoss: 0.185571\n",
            "Train Epoch:3 [15296/60000 (25%)]\tLoss: 0.208075\n",
            "Train Epoch:3 [17216/60000 (29%)]\tLoss: 0.105826\n",
            "Train Epoch:3 [19136/60000 (32%)]\tLoss: 0.269381\n",
            "Train Epoch:3 [21056/60000 (35%)]\tLoss: 0.257290\n",
            "Train Epoch:3 [22976/60000 (38%)]\tLoss: 0.148421\n",
            "Train Epoch:3 [24896/60000 (41%)]\tLoss: 0.284455\n",
            "Train Epoch:3 [26816/60000 (45%)]\tLoss: 0.228394\n",
            "Train Epoch:3 [28736/60000 (48%)]\tLoss: 0.073494\n",
            "Train Epoch:3 [30656/60000 (51%)]\tLoss: 0.220453\n",
            "Train Epoch:3 [32576/60000 (54%)]\tLoss: 0.254119\n",
            "Train Epoch:3 [34496/60000 (57%)]\tLoss: 0.222907\n",
            "Train Epoch:3 [36416/60000 (61%)]\tLoss: 0.089488\n",
            "Train Epoch:3 [38336/60000 (64%)]\tLoss: 0.076226\n",
            "Train Epoch:3 [40256/60000 (67%)]\tLoss: 0.135828\n",
            "Train Epoch:3 [42176/60000 (70%)]\tLoss: 0.116549\n",
            "Train Epoch:3 [44096/60000 (73%)]\tLoss: 0.204810\n",
            "Train Epoch:3 [46016/60000 (77%)]\tLoss: 0.377811\n",
            "Train Epoch:3 [47936/60000 (80%)]\tLoss: 0.208125\n",
            "Train Epoch:3 [49856/60000 (83%)]\tLoss: 0.105911\n",
            "Train Epoch:3 [51776/60000 (86%)]\tLoss: 0.153363\n",
            "Train Epoch:3 [53696/60000 (89%)]\tLoss: 0.106700\n",
            "Train Epoch:3 [55616/60000 (93%)]\tLoss: 0.180044\n",
            "Train Epoch:3 [57536/60000 (96%)]\tLoss: 0.670068\n",
            "Train Epoch:3 [59456/60000 (99%)]\tLoss: 0.215599\n",
            "\n",
            "Test set: Average loss: 0.2868, Accuracy: 9088/10000 (91%)\n",
            "\n",
            "==================================================\n",
            "Train Epoch:4 [1856/60000 (3%)]\tLoss: 0.138266\n",
            "Train Epoch:4 [3776/60000 (6%)]\tLoss: 0.100869\n",
            "Train Epoch:4 [5696/60000 (9%)]\tLoss: 0.345113\n",
            "Train Epoch:4 [7616/60000 (13%)]\tLoss: 0.207033\n",
            "Train Epoch:4 [9536/60000 (16%)]\tLoss: 0.209412\n",
            "Train Epoch:4 [11456/60000 (19%)]\tLoss: 0.212407\n",
            "Train Epoch:4 [13376/60000 (22%)]\tLoss: 0.224177\n",
            "Train Epoch:4 [15296/60000 (25%)]\tLoss: 0.151110\n",
            "Train Epoch:4 [17216/60000 (29%)]\tLoss: 0.288910\n",
            "Train Epoch:4 [19136/60000 (32%)]\tLoss: 0.287291\n",
            "Train Epoch:4 [21056/60000 (35%)]\tLoss: 0.071873\n",
            "Train Epoch:4 [22976/60000 (38%)]\tLoss: 0.097068\n",
            "Train Epoch:4 [24896/60000 (41%)]\tLoss: 0.208669\n",
            "Train Epoch:4 [26816/60000 (45%)]\tLoss: 0.191667\n",
            "Train Epoch:4 [28736/60000 (48%)]\tLoss: 0.257032\n",
            "Train Epoch:4 [30656/60000 (51%)]\tLoss: 0.149455\n",
            "Train Epoch:4 [32576/60000 (54%)]\tLoss: 0.105444\n",
            "Train Epoch:4 [34496/60000 (57%)]\tLoss: 0.182113\n",
            "Train Epoch:4 [36416/60000 (61%)]\tLoss: 0.122724\n",
            "Train Epoch:4 [38336/60000 (64%)]\tLoss: 0.181203\n",
            "Train Epoch:4 [40256/60000 (67%)]\tLoss: 0.123958\n",
            "Train Epoch:4 [42176/60000 (70%)]\tLoss: 0.178896\n",
            "Train Epoch:4 [44096/60000 (73%)]\tLoss: 0.114912\n",
            "Train Epoch:4 [46016/60000 (77%)]\tLoss: 0.170112\n",
            "Train Epoch:4 [47936/60000 (80%)]\tLoss: 0.105763\n",
            "Train Epoch:4 [49856/60000 (83%)]\tLoss: 0.130850\n",
            "Train Epoch:4 [51776/60000 (86%)]\tLoss: 0.116640\n",
            "Train Epoch:4 [53696/60000 (89%)]\tLoss: 0.131550\n",
            "Train Epoch:4 [55616/60000 (93%)]\tLoss: 0.075972\n",
            "Train Epoch:4 [57536/60000 (96%)]\tLoss: 0.119231\n",
            "Train Epoch:4 [59456/60000 (99%)]\tLoss: 0.153699\n",
            "\n",
            "Test set: Average loss: 0.2882, Accuracy: 9101/10000 (91%)\n",
            "\n",
            "==================================================\n",
            "Train Epoch:5 [1856/60000 (3%)]\tLoss: 0.106912\n",
            "Train Epoch:5 [3776/60000 (6%)]\tLoss: 0.393234\n",
            "Train Epoch:5 [5696/60000 (9%)]\tLoss: 0.085769\n",
            "Train Epoch:5 [7616/60000 (13%)]\tLoss: 0.308916\n",
            "Train Epoch:5 [9536/60000 (16%)]\tLoss: 0.190834\n",
            "Train Epoch:5 [11456/60000 (19%)]\tLoss: 0.245870\n",
            "Train Epoch:5 [13376/60000 (22%)]\tLoss: 0.156111\n",
            "Train Epoch:5 [15296/60000 (25%)]\tLoss: 0.138925\n",
            "Train Epoch:5 [17216/60000 (29%)]\tLoss: 0.120931\n",
            "Train Epoch:5 [19136/60000 (32%)]\tLoss: 0.136089\n",
            "Train Epoch:5 [21056/60000 (35%)]\tLoss: 0.422611\n",
            "Train Epoch:5 [22976/60000 (38%)]\tLoss: 0.232185\n",
            "Train Epoch:5 [24896/60000 (41%)]\tLoss: 0.091847\n",
            "Train Epoch:5 [26816/60000 (45%)]\tLoss: 0.114998\n",
            "Train Epoch:5 [28736/60000 (48%)]\tLoss: 0.210360\n",
            "Train Epoch:5 [30656/60000 (51%)]\tLoss: 0.177276\n",
            "Train Epoch:5 [32576/60000 (54%)]\tLoss: 0.152346\n",
            "Train Epoch:5 [34496/60000 (57%)]\tLoss: 0.239885\n",
            "Train Epoch:5 [36416/60000 (61%)]\tLoss: 0.162573\n",
            "Train Epoch:5 [38336/60000 (64%)]\tLoss: 0.164200\n",
            "Train Epoch:5 [40256/60000 (67%)]\tLoss: 0.159976\n",
            "Train Epoch:5 [42176/60000 (70%)]\tLoss: 0.093832\n",
            "Train Epoch:5 [44096/60000 (73%)]\tLoss: 0.285002\n",
            "Train Epoch:5 [46016/60000 (77%)]\tLoss: 0.190728\n",
            "Train Epoch:5 [47936/60000 (80%)]\tLoss: 0.166342\n",
            "Train Epoch:5 [49856/60000 (83%)]\tLoss: 0.198454\n",
            "Train Epoch:5 [51776/60000 (86%)]\tLoss: 0.143425\n",
            "Train Epoch:5 [53696/60000 (89%)]\tLoss: 0.088827\n",
            "Train Epoch:5 [55616/60000 (93%)]\tLoss: 0.357420\n",
            "Train Epoch:5 [57536/60000 (96%)]\tLoss: 0.339652\n",
            "Train Epoch:5 [59456/60000 (99%)]\tLoss: 0.178496\n",
            "\n",
            "Test set: Average loss: 0.3312, Accuracy: 9019/10000 (90%)\n",
            "\n",
            "==================================================\n",
            "Train Epoch:6 [1856/60000 (3%)]\tLoss: 0.102721\n",
            "Train Epoch:6 [3776/60000 (6%)]\tLoss: 0.165432\n",
            "Train Epoch:6 [5696/60000 (9%)]\tLoss: 0.134973\n",
            "Train Epoch:6 [7616/60000 (13%)]\tLoss: 0.181653\n",
            "Train Epoch:6 [9536/60000 (16%)]\tLoss: 0.201437\n",
            "Train Epoch:6 [11456/60000 (19%)]\tLoss: 0.181881\n",
            "Train Epoch:6 [13376/60000 (22%)]\tLoss: 0.149521\n",
            "Train Epoch:6 [15296/60000 (25%)]\tLoss: 0.155964\n",
            "Train Epoch:6 [17216/60000 (29%)]\tLoss: 0.166312\n",
            "Train Epoch:6 [19136/60000 (32%)]\tLoss: 0.087662\n",
            "Train Epoch:6 [21056/60000 (35%)]\tLoss: 0.142783\n",
            "Train Epoch:6 [22976/60000 (38%)]\tLoss: 0.253456\n",
            "Train Epoch:6 [24896/60000 (41%)]\tLoss: 0.085645\n",
            "Train Epoch:6 [26816/60000 (45%)]\tLoss: 0.389864\n",
            "Train Epoch:6 [28736/60000 (48%)]\tLoss: 0.174609\n",
            "Train Epoch:6 [30656/60000 (51%)]\tLoss: 0.227990\n",
            "Train Epoch:6 [32576/60000 (54%)]\tLoss: 0.366779\n",
            "Train Epoch:6 [34496/60000 (57%)]\tLoss: 0.187258\n",
            "Train Epoch:6 [36416/60000 (61%)]\tLoss: 0.052983\n",
            "Train Epoch:6 [38336/60000 (64%)]\tLoss: 0.095592\n",
            "Train Epoch:6 [40256/60000 (67%)]\tLoss: 0.222340\n",
            "Train Epoch:6 [42176/60000 (70%)]\tLoss: 0.094148\n",
            "Train Epoch:6 [44096/60000 (73%)]\tLoss: 0.165369\n",
            "Train Epoch:6 [46016/60000 (77%)]\tLoss: 0.125226\n",
            "Train Epoch:6 [47936/60000 (80%)]\tLoss: 0.154778\n",
            "Train Epoch:6 [49856/60000 (83%)]\tLoss: 0.084000\n",
            "Train Epoch:6 [51776/60000 (86%)]\tLoss: 0.131964\n",
            "Train Epoch:6 [53696/60000 (89%)]\tLoss: 0.174402\n",
            "Train Epoch:6 [55616/60000 (93%)]\tLoss: 0.212982\n",
            "Train Epoch:6 [57536/60000 (96%)]\tLoss: 0.128917\n",
            "Train Epoch:6 [59456/60000 (99%)]\tLoss: 0.238790\n",
            "\n",
            "Test set: Average loss: 0.3139, Accuracy: 9102/10000 (91%)\n",
            "\n",
            "==================================================\n",
            "Train Epoch:7 [1856/60000 (3%)]\tLoss: 0.096243\n",
            "Train Epoch:7 [3776/60000 (6%)]\tLoss: 0.110103\n",
            "Train Epoch:7 [5696/60000 (9%)]\tLoss: 0.182065\n",
            "Train Epoch:7 [7616/60000 (13%)]\tLoss: 0.348474\n",
            "Train Epoch:7 [9536/60000 (16%)]\tLoss: 0.130800\n",
            "Train Epoch:7 [11456/60000 (19%)]\tLoss: 0.090113\n",
            "Train Epoch:7 [13376/60000 (22%)]\tLoss: 0.165278\n",
            "Train Epoch:7 [15296/60000 (25%)]\tLoss: 0.140494\n",
            "Train Epoch:7 [17216/60000 (29%)]\tLoss: 0.116634\n",
            "Train Epoch:7 [19136/60000 (32%)]\tLoss: 0.225280\n",
            "Train Epoch:7 [21056/60000 (35%)]\tLoss: 0.083609\n",
            "Train Epoch:7 [22976/60000 (38%)]\tLoss: 0.106249\n",
            "Train Epoch:7 [24896/60000 (41%)]\tLoss: 0.233350\n",
            "Train Epoch:7 [26816/60000 (45%)]\tLoss: 0.251051\n",
            "Train Epoch:7 [28736/60000 (48%)]\tLoss: 0.130716\n",
            "Train Epoch:7 [30656/60000 (51%)]\tLoss: 0.150706\n",
            "Train Epoch:7 [32576/60000 (54%)]\tLoss: 0.324833\n",
            "Train Epoch:7 [34496/60000 (57%)]\tLoss: 0.267631\n",
            "Train Epoch:7 [36416/60000 (61%)]\tLoss: 0.026937\n",
            "Train Epoch:7 [38336/60000 (64%)]\tLoss: 0.154308\n",
            "Train Epoch:7 [40256/60000 (67%)]\tLoss: 0.198371\n",
            "Train Epoch:7 [42176/60000 (70%)]\tLoss: 0.019845\n",
            "Train Epoch:7 [44096/60000 (73%)]\tLoss: 0.290985\n",
            "Train Epoch:7 [46016/60000 (77%)]\tLoss: 0.133249\n",
            "Train Epoch:7 [47936/60000 (80%)]\tLoss: 0.083056\n",
            "Train Epoch:7 [49856/60000 (83%)]\tLoss: 0.147549\n",
            "Train Epoch:7 [51776/60000 (86%)]\tLoss: 0.207536\n",
            "Train Epoch:7 [53696/60000 (89%)]\tLoss: 0.104392\n",
            "Train Epoch:7 [55616/60000 (93%)]\tLoss: 0.123439\n",
            "Train Epoch:7 [57536/60000 (96%)]\tLoss: 0.100982\n",
            "Train Epoch:7 [59456/60000 (99%)]\tLoss: 0.073228\n",
            "\n",
            "Test set: Average loss: 0.3277, Accuracy: 9080/10000 (91%)\n",
            "\n",
            "==================================================\n",
            "Train Epoch:8 [1856/60000 (3%)]\tLoss: 0.199477\n",
            "Train Epoch:8 [3776/60000 (6%)]\tLoss: 0.196423\n",
            "Train Epoch:8 [5696/60000 (9%)]\tLoss: 0.199938\n",
            "Train Epoch:8 [7616/60000 (13%)]\tLoss: 0.156799\n",
            "Train Epoch:8 [9536/60000 (16%)]\tLoss: 0.165620\n",
            "Train Epoch:8 [11456/60000 (19%)]\tLoss: 0.076162\n",
            "Train Epoch:8 [13376/60000 (22%)]\tLoss: 0.121388\n",
            "Train Epoch:8 [15296/60000 (25%)]\tLoss: 0.072354\n",
            "Train Epoch:8 [17216/60000 (29%)]\tLoss: 0.090146\n",
            "Train Epoch:8 [19136/60000 (32%)]\tLoss: 0.124931\n",
            "Train Epoch:8 [21056/60000 (35%)]\tLoss: 0.164884\n",
            "Train Epoch:8 [22976/60000 (38%)]\tLoss: 0.093898\n",
            "Train Epoch:8 [24896/60000 (41%)]\tLoss: 0.132817\n",
            "Train Epoch:8 [26816/60000 (45%)]\tLoss: 0.227525\n",
            "Train Epoch:8 [28736/60000 (48%)]\tLoss: 0.257341\n",
            "Train Epoch:8 [30656/60000 (51%)]\tLoss: 0.113242\n",
            "Train Epoch:8 [32576/60000 (54%)]\tLoss: 0.284743\n",
            "Train Epoch:8 [34496/60000 (57%)]\tLoss: 0.124768\n",
            "Train Epoch:8 [36416/60000 (61%)]\tLoss: 0.182717\n",
            "Train Epoch:8 [38336/60000 (64%)]\tLoss: 0.108198\n",
            "Train Epoch:8 [40256/60000 (67%)]\tLoss: 0.197850\n",
            "Train Epoch:8 [42176/60000 (70%)]\tLoss: 0.078779\n",
            "Train Epoch:8 [44096/60000 (73%)]\tLoss: 0.338784\n",
            "Train Epoch:8 [46016/60000 (77%)]\tLoss: 0.181736\n",
            "Train Epoch:8 [47936/60000 (80%)]\tLoss: 0.331163\n",
            "Train Epoch:8 [49856/60000 (83%)]\tLoss: 0.060909\n",
            "Train Epoch:8 [51776/60000 (86%)]\tLoss: 0.124608\n",
            "Train Epoch:8 [53696/60000 (89%)]\tLoss: 0.054828\n",
            "Train Epoch:8 [55616/60000 (93%)]\tLoss: 0.218123\n",
            "Train Epoch:8 [57536/60000 (96%)]\tLoss: 0.285258\n",
            "Train Epoch:8 [59456/60000 (99%)]\tLoss: 0.103627\n",
            "\n",
            "Test set: Average loss: 0.3236, Accuracy: 9050/10000 (90%)\n",
            "\n",
            "==================================================\n",
            "Train Epoch:9 [1856/60000 (3%)]\tLoss: 0.207156\n",
            "Train Epoch:9 [3776/60000 (6%)]\tLoss: 0.135622\n",
            "Train Epoch:9 [5696/60000 (9%)]\tLoss: 0.124898\n",
            "Train Epoch:9 [7616/60000 (13%)]\tLoss: 0.054496\n",
            "Train Epoch:9 [9536/60000 (16%)]\tLoss: 0.186907\n",
            "Train Epoch:9 [11456/60000 (19%)]\tLoss: 0.272303\n",
            "Train Epoch:9 [13376/60000 (22%)]\tLoss: 0.113659\n",
            "Train Epoch:9 [15296/60000 (25%)]\tLoss: 0.106145\n",
            "Train Epoch:9 [17216/60000 (29%)]\tLoss: 0.129949\n",
            "Train Epoch:9 [19136/60000 (32%)]\tLoss: 0.184738\n",
            "Train Epoch:9 [21056/60000 (35%)]\tLoss: 0.053210\n",
            "Train Epoch:9 [22976/60000 (38%)]\tLoss: 0.260108\n",
            "Train Epoch:9 [24896/60000 (41%)]\tLoss: 0.141786\n",
            "Train Epoch:9 [26816/60000 (45%)]\tLoss: 0.141921\n",
            "Train Epoch:9 [28736/60000 (48%)]\tLoss: 0.183118\n",
            "Train Epoch:9 [30656/60000 (51%)]\tLoss: 0.201850\n",
            "Train Epoch:9 [32576/60000 (54%)]\tLoss: 0.076606\n",
            "Train Epoch:9 [34496/60000 (57%)]\tLoss: 0.137024\n",
            "Train Epoch:9 [36416/60000 (61%)]\tLoss: 0.064091\n",
            "Train Epoch:9 [38336/60000 (64%)]\tLoss: 0.182440\n",
            "Train Epoch:9 [40256/60000 (67%)]\tLoss: 0.076878\n",
            "Train Epoch:9 [42176/60000 (70%)]\tLoss: 0.266878\n",
            "Train Epoch:9 [44096/60000 (73%)]\tLoss: 0.163055\n",
            "Train Epoch:9 [46016/60000 (77%)]\tLoss: 0.206575\n",
            "Train Epoch:9 [47936/60000 (80%)]\tLoss: 0.095719\n",
            "Train Epoch:9 [49856/60000 (83%)]\tLoss: 0.076882\n",
            "Train Epoch:9 [51776/60000 (86%)]\tLoss: 0.187103\n",
            "Train Epoch:9 [53696/60000 (89%)]\tLoss: 0.082735\n",
            "Train Epoch:9 [55616/60000 (93%)]\tLoss: 0.159877\n",
            "Train Epoch:9 [57536/60000 (96%)]\tLoss: 0.285835\n",
            "Train Epoch:9 [59456/60000 (99%)]\tLoss: 0.193807\n",
            "\n",
            "Test set: Average loss: 0.3070, Accuracy: 9071/10000 (91%)\n",
            "\n",
            "==================================================\n",
            "Train Epoch:10 [1856/60000 (3%)]\tLoss: 0.061301\n",
            "Train Epoch:10 [3776/60000 (6%)]\tLoss: 0.187269\n",
            "Train Epoch:10 [5696/60000 (9%)]\tLoss: 0.281981\n",
            "Train Epoch:10 [7616/60000 (13%)]\tLoss: 0.223981\n",
            "Train Epoch:10 [9536/60000 (16%)]\tLoss: 0.173252\n",
            "Train Epoch:10 [11456/60000 (19%)]\tLoss: 0.029414\n",
            "Train Epoch:10 [13376/60000 (22%)]\tLoss: 0.182369\n",
            "Train Epoch:10 [15296/60000 (25%)]\tLoss: 0.416485\n",
            "Train Epoch:10 [17216/60000 (29%)]\tLoss: 0.080870\n",
            "Train Epoch:10 [19136/60000 (32%)]\tLoss: 0.233790\n",
            "Train Epoch:10 [21056/60000 (35%)]\tLoss: 0.091536\n",
            "Train Epoch:10 [22976/60000 (38%)]\tLoss: 0.179903\n",
            "Train Epoch:10 [24896/60000 (41%)]\tLoss: 0.117058\n",
            "Train Epoch:10 [26816/60000 (45%)]\tLoss: 0.170653\n",
            "Train Epoch:10 [28736/60000 (48%)]\tLoss: 0.194984\n",
            "Train Epoch:10 [30656/60000 (51%)]\tLoss: 0.193652\n",
            "Train Epoch:10 [32576/60000 (54%)]\tLoss: 0.085984\n",
            "Train Epoch:10 [34496/60000 (57%)]\tLoss: 0.089510\n",
            "Train Epoch:10 [36416/60000 (61%)]\tLoss: 0.094291\n",
            "Train Epoch:10 [38336/60000 (64%)]\tLoss: 0.026766\n",
            "Train Epoch:10 [40256/60000 (67%)]\tLoss: 0.201674\n",
            "Train Epoch:10 [42176/60000 (70%)]\tLoss: 0.107978\n",
            "Train Epoch:10 [44096/60000 (73%)]\tLoss: 0.152084\n",
            "Train Epoch:10 [46016/60000 (77%)]\tLoss: 0.214671\n",
            "Train Epoch:10 [47936/60000 (80%)]\tLoss: 0.046876\n",
            "Train Epoch:10 [49856/60000 (83%)]\tLoss: 0.171446\n",
            "Train Epoch:10 [51776/60000 (86%)]\tLoss: 0.080365\n",
            "Train Epoch:10 [53696/60000 (89%)]\tLoss: 0.147970\n",
            "Train Epoch:10 [55616/60000 (93%)]\tLoss: 0.176239\n",
            "Train Epoch:10 [57536/60000 (96%)]\tLoss: 0.176976\n",
            "Train Epoch:10 [59456/60000 (99%)]\tLoss: 0.173135\n",
            "\n",
            "Test set: Average loss: 0.3122, Accuracy: 9112/10000 (91%)\n",
            "\n",
            "==================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "코드 참조: https://ingu627.github.io/code/alexnet_pytorch/"
      ],
      "metadata": {
        "id": "jQVXVt37gq0L"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "stQM8dJPfqVX"
      },
      "execution_count": 13,
      "outputs": []
    }
  ]
}